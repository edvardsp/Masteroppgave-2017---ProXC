% !TEX encoding = UTF-8 Unicode
%!TEX root = main.tex
% !TEX spellcheck = en-US
%%=========================================

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{ch:introduction}

Computers are for many the greatest engineering feat in the 20th century, taking into consideration the vast complexity, precision, and knowledge it has required to reach the state of computers we have today. Since its inception with the transistor in the 40s, the computer has been subjected to numerous changes in different technology to further increase the performance. Such technology includes the decrease in size of transistors down to nanometers, faster clock rates for processors, and most notably the transition from single\hyp{}core to multi\hyp{}core architectures.

When entering the 21st century, the main drive behind increasing processor performance was increasing the transistor count in the processor, following the exponential growth described by Moore's law for decades. However, since 2005 this strategy was no longer sustainable. The physical limitations of the Dennard scaling were starting to show, and a shift from single\hyp{}core towards multi\hyp{}core architectures was a response to this limitation. This shift is, of course, an oversimplification of the issue, but multi\hyp{}core architectures are pretty much the norm among desktop processors in this day and age.

With the transition to multi\hyp{}core architectures, software can no longer na√Øvely rely on increase in processor performance for an increase in software performance. Software has to now exploit the parallelism in multi\hyp{}core processors, which requires concurrent programming. Concurrent programming has long existed before multi\hyp{}core processors, but is now the main tool for developers to write scalable software which can utilize the parallel power in multi\hyp{}core architectures.

Concurrent programming revolves around having multiple threads of execution running concurrently (simultaneously) in a program. Such programs are called concurrent systems. Even though concurrent programming is a powerful and expressive paradigm to write software in, it also is difficult to reason whether a concurrent system behaves as specified by the programmer. What makes it hard to write correct and well\hyp{}behaved concurrent systems has to do with concurrency adding significant complexity to the system. Additionally, it is an added mental overhead for the programmer. Even the simplest and most subtle errors in a program can explode into the most obscure and hard\hyp{}to\hyp{}track bugs. Being able to write expressive concurrent systems, as well as being able to reason about the correctness of the system, are probably the biggest challenges with concurrency.

\textit{Communicating Sequential Processes} (CSP) was an effort by \citet{hoare1978communicating} to harness the expressiveness of concurrent programming while being able to prove the correctness of such models. CSP is by its own a formal language used to describe concurrent models. These models described by CSP is a parallel composition of sequential processes, which only communicates through mutually agreed message\hyp{}passing constructs. This inherently inhibits any types of race\hyp{}conditions with shared memory to ever occur. However, the real power of CSP comes from the ability to reason about the correctness of these models, such as the absence of deadlocks and livelocks.  

Several programming languages has incorporated CSP formalism into the language, such as occam-$\pi$ \citep{barrett1992occam}, XC \citep{douglas2009programming} and Go \citep{go2009go}, but the mainstream popularity has not been great; Go is an exception, which ranks as one of the most popular languages to date. As a response, a collection of transpilers\footnote{Source\hyp{}to\hyp{}source compiler, compiling source code written in one language to source code in another language} and libraries has been created for more popular and well\hyp{}established programming languages, such as C \citep{pettersen2016proxc}, \Cpp{} \citep{brown2003c++csp,brown2007c++csp2,chalmers2016cppcsp}, Python \citep{bjorndalen2007pycsp}, and Java \citep{welch2007jcsp}. 

Concurrent systems written in CSP has a great potential for exploiting the parallelism on multi\hyp{}core architectures, as CSP is inherently parallel. CSP also has the added benefit of providing a model which can be reasoned about the correctness of the behaviour. However, not many CSP\hyp{}based frameworks takes advantage of the potential parallelism in multi\hyp{}core architectures. Existing libraries are either outdated for modern use, or does not satisfy performance wise. \Cref{ch:multicore_csp} takes this discussion further and argues there is a need for a modern solution.

With the lack of a modern CSP library for C, ProXC was developed as a result of the work done by \citet{pettersen2016proxc}. ProXC aimed to show the possibilities such a library could have. The focus of ProXC was however on the provided abstractions rather on performance, and support for multi\hyp{}core architectures was not implemented. In this thesis, the work done on ProXC is continued with the aim to provide a portable CSP library for multi\hyp{}core architectures. 

This thesis presents the work and results of \Proxc{}, a portable CSP library for modern \Cpp{} with support for multi\hyp{}core architectures. \Proxc{} is a concurrency library which uses dynamic multithreading combined with lightweight processes to achieve proper parallel computing on multi\hyp{}core architectures. The runtime system employed by \Proxc{} consists of a number of schedulers equal to the number of available logical processor cores. Each scheduler is responsible for scheduling and running these lightweight processes on a kernel\hyp{}thread, and uses work stealing to load balance work between schedulers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Status of ProXC}

As this thesis is a continuation of the work done in the project thesis by \citet{pettersen2016proxc}, a short status report of ProXC the project is presented.

As of writing this thesis, ProXC has not undergone any major development, except for some bug fixes. The API has remained unchanged, as well as any of the major issues pinpointed by the project thesis. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Thesis Structure}


The thesis is structured into three parts. \Cref{part:preliminaries} discusses the theoretical basis of this thesis, as well as arguing the motivation behind CSP with multi\hyp{}core support: \Cref{ch:theoretical_background} gives an introduction to concurrent programming, and details all relevant theoretical knowledge for which this thesis is based on. \Cref{ch:multicore_csp} argues there is a motivation for a CSP library with multi\hyp{}core support, and provides a summary of existing solutions and how they work.

\Cref{part:proxc++} details the work regarding \Proxc{}: \Cref{ch:library_specifications} details the library specification. \Cref{ch:design} and \Cref{ch:implementation} presents respectively the design and implementation. \Cref{ch:examples_usage} presents examples of how \Proxc{} is used along with code examples. \Cref{ch:performance} performs a benchmark of \Proxc{} compared to existing solutions, both highlighting the difference between single\hyp{}core and multi\hyp{}core implementations.

\Cref{part:discussions} discusses different aspects of CSP and multi\hyp{}core, limitation and uses of \Proxc{}, and concludes the thesis: \Cref{ch:proxc++_vs_proxc} compares both projects, drawing the differences and similarities between the multi\hyp{}core and single\hyp{}core library. \Cref{ch:difficulty_multicore_csp} explains the challenges with implementing a CSP framework with multi\hyp{}core support. \Cref{ch:shortcomings_limitations} discusses the limitations and uses of \Proxc{}. \Cref{ch:future_work} lists a set of potential future work. \Cref{ch:conclusion} draws a conclusion for the thesis.


Lastly, the appendices list the used acronyms (\cref{ch:acronyms}), and a complete listing of all benchmarks tested in \cref{ch:performance} (\cref{ch:benchmark_code}). The last section in the thesis is the list of references.
