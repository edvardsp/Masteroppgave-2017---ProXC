% !TEX encoding = UTF-8 Unicode
%!TEX root = main.tex
% !TEX spellcheck = en-US
%%=========================================


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{CSP + Multicore = True ?}
\label{ch:multicore_csp}


This chapter takes the motivation described in the introduction and elaborates on why a multicore CSP library would be desirable. Further, a review of existing solutions of multicore CSP programming languages and libraries is presented and how they work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation behind Multicore CSP}
\label{sec:multicore_csp_motivation}


As mentioned in the introduction, software can no longer rely on increase in processor performance for an increase in software performance. High performance software which wishes to utilize the full potential of multicore processors needs to do two things: identify the parallel units of work in a program, and efficiently run the parallel work using the available processor resources. 

Why is it important to identify parallel units of work in a program? The short answer is Amdalh's law \citep{amdahl1967validity}. Amdalh's law argues the maximum speedup in a program with infinite number of parallel processors is limited to $^1/_s$, given the fraction of sequential work in the program is $s$. If a program has a fraction of $50\%$ sequential work the maximum speedup possible is $2$, and a fraction of $25\%$ sequential work yields a maximum speedup possible of $4$. There exists more refined models of Amdalh's law developed for different multicore architectures \citep{sun2010reevaluating}, but the point is the same; parallelism is key.

If a programmer cannot reduce the fraction of sequential work, or in other words cannot increase the fraction of parallel work, there is not much to gain from multicore processors. Increasing the fraction of parallel work usually consists of identifying parallelism in sequential work, and subsequently converting said work to parallel units of work. Identifying parallelism is usually the easiest part, e.g. loops and decoupled sections of code are easy to identify and reason whether is parallel or not. How to convert sequential work to parallel work is however another matter, requiring details of scheduling and synchronization.

When parallel work has been identified, how do you efficiently schedule and run the parallel work on the available processor resources? When you want the program to be scalable for all multicore processors, simply hard coding for a given multicore architecture will not suffice. Dynamic multithreading is a solution, which has strategies for dynamically distribute parallel work among available processor cores. Strategies include work stealing and work sharing, but the core philosophy is letting the dynamic multithreading take care of scheduling and synchronization, while the programmer specifies what the parallel work is. Whether dynamic multithreading is implemented with the program or as part of a runtime system underneath achieves the same goal. 

This thesis argues CSP is a good candidate for creating high performance software for multicore processors. First of all, CSP provides expressive and safe abstractions for creating concurrent systems, which also can be statically reasoned whether certain specifications and safety properties are met. Secondly, CSP inherently defines its parallel units of work, as all CSP models are a parallel composition of sequential processes. This inherent parallel nature of CSP essentially identifies all higher level units of parallel work in a program. Now, if a dynamic multithreading scheme was employed together with a CSP framework, it would enable exploiting the parallel nature of multicore processors.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Existing Solutions of Dynamic Multithreaded CSP}
\label{sec:multicore_csp_existing}


Combining CSP with dynamic multithreading is no new invention. Even tough dynamic multithreading has usually not been the focus with CSP framework, multiple frameworks does exists with dynamic multithreading, including programming languages and programming libraries. The frameworks varies in availability and age, ranging from proprietary programming languages to open\hyp{}source libraries. Below is a non\hyp{}exhaustive list of dynamic multithreaded CSP frameworks summarized.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Programming Languages}


Not many programming languages has made concurrency a native part of the language design, yet alone CSP\hyp{}based concurrency. For those languages with CSP as a native influence, dynamic multithreading has usually not been the focus. The first CSP\hyp{}based languages was designed for microprocessor hardware, and with the years transitioned to general\hyp{}purpose programming languages. Below are the three most influential CSP\hyp{}based programming languages with dynamic multithreading presented.


\subsubsection{occam on the Transputer}


\subsubsection{XC on XMOS}


\subsubsection{Go}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Programming Libraries}


Several CSP\hyp{}based concurrency libraries has been developed for more popular and established programming languages not supporting CSP natively. Below are the most influential CSP\hyp{}based concurrency libraries with dynamic multithreading presented.


\subsubsection{C++CSP2}


\subsubsection{C++CSP}


\subsubsection{Boost.Fiber}

